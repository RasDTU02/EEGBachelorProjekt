{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.cm as cm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "import seaborn as sns\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from collections import Counter\n",
    "from matplotlib.ticker import FixedLocator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import random\n",
    "from scipy.stats import ttest_ind\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = 'C:\\\\Users\\\\RJEN0307\\\\Desktop\\\\Rasmus, Bachelorprojekt\\\\Data\\\\1_csv'\n",
    "\n",
    "patient_numbers = []\n",
    "patient_data = {}\n",
    "\n",
    "for file in os.listdir(full_path):\n",
    "    filename = os.fsdecode(file)\n",
    "    patient_number = filename.split('_')[0] \n",
    "    patient_numbers.append(patient_number)\n",
    "    patient_file_dir = os.path.join(full_path, filename)\n",
    "    data = pd.read_csv(patient_file_dir)\n",
    "\n",
    "    data.rename(columns={'Unnamed: 0': 'Index'}, inplace=True)\n",
    "    data['Event'] = data['Event'].map({'R': 0, 'M': 1, 'F': 2})\n",
    "\n",
    "    columns_to_remove = [col for col in data.columns if '_N' in col]\n",
    "    data = data.drop(columns=columns_to_remove)\n",
    "\n",
    "    resting_data = data[data['Event'] == 0]\n",
    "    moving_data = data[data['Event'] == 1]\n",
    "    familiar_data = data[data['Event'] == 2]\n",
    "    target_count = min(moving_data.shape[0], familiar_data.shape[0])\n",
    "    resting_data = resting_data.iloc[-target_count:]\n",
    "    resting_data = resting_data.reset_index(drop=True)\n",
    "    moving_data = moving_data.reset_index(drop=True)\n",
    "    familiar_data = familiar_data.reset_index(drop=True)\n",
    "    balanced_data = pd.concat([resting_data, moving_data, familiar_data]).reset_index(drop=True)\n",
    "\n",
    "    standarize_list = ['PSD Delta', 'PSD Theta',  'PSD Alpha', 'PSD Beta', 'PSD Gamma', 'PSD SE', 'PSD MSF', 'PSD Sef90', 'PSD Sef95', 'PE', 'wSMI', 'Kolmogorov', 'Freq_Slope mean', 'Freq_Slope std']\n",
    "    \n",
    "    #standarize_list =  ['PSD Delta', 'PSD Theta', 'PSD Alpha', 'PSD Beta', 'PSD Gamma']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    balanced_data[standarize_list] = scaler.fit_transform(balanced_data[standarize_list])\n",
    "    patient_data[patient_number] = balanced_data\n",
    "    \n",
    "    # show patient 10\n",
    "    print(patient_data['p10'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_pca_components(patient_data):\n",
    "    component_counts = []\n",
    "    cumulative_variances = []\n",
    "    for patient, data in patient_data.items():\n",
    "        features = data.drop(columns=['Index', 'Event'])\n",
    "\n",
    "        pca = PCA()\n",
    "        pca.fit(features)\n",
    "        cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "        cumulative_variances.append(cumulative_variance)\n",
    "        n_components = next(i for i, v in enumerate(cumulative_variance) if v >= 0.95) + 1\n",
    "        component_counts.append(n_components)\n",
    "\n",
    "    most_common_components = Counter(component_counts).most_common(1)[0][0]\n",
    "    return most_common_components, component_counts, cumulative_variances\n",
    "\n",
    "most_common_components, component_counts, cumulative_variances = find_optimal_pca_components(patient_data)\n",
    "print(f\"Most common number of PCA components explaining 95% variance: {most_common_components}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cumulative_variance in cumulative_variances:\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, alpha=0.3)\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Explained Variance for All Patients')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.axvline(x=most_common_components, color='b', linestyle='--', label=f'Optimal Components: {most_common_components}')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_plot_for_each_patient(patient_data, patients, n_components_pca=5, n_components_gmm=3, save_to_pdf=False, output_pdf_path='combined_patient_plots_final.pdf', contribution_pdf_path='feature_contribution_plot.pdf',output_pca_path = 'PCAplot_1',pca_plot = False,feature_importance=False):\n",
    "    feature_names = ['PSD Delta', 'PSD Theta', 'PSD Alpha', 'PSD Beta', 'PSD Gamma', 'PSD SE', 'PSD MSF', 'PSD Sef90', 'PSD Sef95', 'PE', 'wSMI', 'Kolmogorov', 'Freq_Slope mean', 'Freq_Slope std']\n",
    "    #feature_names =  ['PSD Delta', 'PSD Theta', 'PSD Alpha', 'PSD Beta', 'PSD Gamma']\n",
    "    \n",
    "    if save_to_pdf:\n",
    "        pdf = PdfPages(output_pdf_path)\n",
    "    \n",
    "    feature_importance_count = {feature: 0 for feature in feature_names}\n",
    "    all_pca_data = []\n",
    "    all_patient_labels = []\n",
    "\n",
    "    \n",
    "    for patient in patients:\n",
    "        data = patient_data[patient].copy()\n",
    "\n",
    "        data_for_pca = data.drop(columns=['Event'])\n",
    "\n",
    "        pca = PCA(n_components=n_components_pca)\n",
    "        reduced_data = pca.fit_transform(data_for_pca)\n",
    "        pca_df = pd.DataFrame(reduced_data, columns=[f'PC{i+1}' for i in range(n_components_pca)])\n",
    "\n",
    "        all_pca_data.append(pca_df)\n",
    "        all_patient_labels.extend([patient] * len(reduced_data))\n",
    "        \n",
    "        \n",
    "        gmm = GaussianMixture(n_components=n_components_gmm, random_state=0)\n",
    "        gmm.fit(reduced_data)\n",
    "        labels = gmm.predict(reduced_data)\n",
    "        \n",
    "        feature_contributions = np.sum(np.abs(pca.components_[:2]), axis=0)\n",
    "        feature_contributions /= np.sum(feature_contributions)\n",
    "\n",
    "        if len(feature_contributions) != len(feature_names):\n",
    "            feature_contributions = feature_contributions[:len(feature_names)]\n",
    "\n",
    "        importance_threshold = np.percentile(feature_contributions, 75)\n",
    "\n",
    "        for idx, contribution in enumerate(feature_contributions):\n",
    "            if contribution >= importance_threshold:\n",
    "                feature_importance_count[feature_names[idx]] += 1\n",
    "\n",
    "        patient_data_with_labels = data.copy()\n",
    "        patient_data_with_labels['Cluster Label'] = labels\n",
    "\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(15, 15))\n",
    "        \n",
    "        ax = axes[0]\n",
    "        cluster_colors = ['firebrick' if label == 0 else 'darkblue' if label == 1 else 'dodgerblue' for label in labels]\n",
    "\n",
    "\n",
    "        scatter = ax.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_colors, marker='o', edgecolor='black', linewidth=0.5, s=30)\n",
    "        ax.scatter(gmm.means_[:, 0], gmm.means_[:, 1], s=100, c='black', marker='x', label='Cluster Means')\n",
    "        ax.set_title(f\"Clusters for Patient {patient}\", fontsize=10)\n",
    "        ax.set_xlabel('PCA Component 1')\n",
    "        ax.set_ylabel('PCA Component 2')\n",
    "        ax.grid(False)\n",
    "        \n",
    "        ax2 = axes[1]\n",
    "        time = np.arange(len(patient_data_with_labels))\n",
    "        \n",
    "        ax2.plot(time, patient_data_with_labels['PSD Delta'], label='Delta (0.5-4 Hz)', color='black', linewidth=1)\n",
    "        ax2.plot(time, patient_data_with_labels['PSD Theta'], label='Theta (4-8 Hz)', color='gray', linewidth=1)\n",
    "        ax2.plot(time, patient_data_with_labels['PSD Beta'], label='Beta (12-30 Hz)', color='darkgray', linewidth=1)\n",
    "        ax2.plot(time, patient_data_with_labels['PSD Gamma'], label='Gamma (>30 Hz)', color='dimgray', linewidth=1)\n",
    "\n",
    "        ax2.scatter(time, patient_data_with_labels['PSD Delta'], c=cluster_colors, s=50, edgecolor='black', label='Cluster Data Points (Delta)', zorder=3)\n",
    "        ax2.scatter(time, patient_data_with_labels['PSD Theta'], c=cluster_colors, s=50, edgecolor='black', label='Cluster Data Points (Theta)', zorder=3)\n",
    "        ax2.scatter(time, patient_data_with_labels['PSD Beta'], c=cluster_colors, s=50, edgecolor='black', label='Cluster Data Points (Beta)', zorder=3)\n",
    "        ax2.scatter(time, patient_data_with_labels['PSD Gamma'], c=cluster_colors, s=50, edgecolor='black', label='Cluster Data Points (Gamma)', zorder=3)\n",
    "\n",
    "        stimuli_colors = {0: 'lightcoral', 1: 'lightblue', 2: 'lightgreen'}  # 0=R, 1=M, 2=F\n",
    "        \n",
    "        event_groups = patient_data_with_labels['Event'].ne(patient_data_with_labels['Event'].shift()).cumsum()\n",
    "        \n",
    "        for _, group_data in patient_data_with_labels.groupby(event_groups):\n",
    "            stimulus_value = group_data['Event'].iloc[0]\n",
    "            if stimulus_value in stimuli_colors:\n",
    "                start_idx = group_data.index[0]\n",
    "                end_idx = group_data.index[-1]\n",
    "                start_time = time[start_idx]\n",
    "                end_time = time[end_idx]\n",
    "                ax2.axvspan(start_time, end_time, color=stimuli_colors[stimulus_value], alpha=0.3, zorder=1)\n",
    "        \n",
    "        ax2.set_title(f'EEG Features with Clusters for Patient {patient}', fontsize=10)\n",
    "        ax2.set_xlabel('Sample Index')\n",
    "        ax2.set_ylabel('Feature Value')\n",
    "        ax2.grid(False)\n",
    "\n",
    "        ax3 = axes[2]\n",
    "        sns.barplot(x=feature_names[:len(feature_contributions)], y=feature_contributions, palette='viridis', ax=ax3, hue=feature_names[:len(feature_contributions)], legend=False)\n",
    "        ax3.xaxis.set_major_locator(FixedLocator(range(len(feature_names[:len(feature_contributions)]))))\n",
    "        ax3.set_xticklabels(feature_names[:len(feature_contributions)], rotation=90)\n",
    "        ax3.set_xlabel('Features')\n",
    "        ax3.set_ylabel('Contribution')\n",
    "        ax3.set_title(f'Feature Contribution for Patient {patient} (PCA Loadings)')\n",
    "        \n",
    "\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_to_pdf:\n",
    "            pdf.savefig(fig)\n",
    "            plt.close(fig)\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "    if save_to_pdf:\n",
    "        pdf.close()\n",
    "\n",
    "    if feature_importance:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=list(feature_importance_count.keys()), y=list(feature_importance_count.values()), palette='viridis', hue=list(feature_importance_count.keys()), legend=False)\n",
    "\n",
    "        plt.xlabel('Features')\n",
    "        plt.ylabel('Count of Importance')\n",
    "        plt.title('Feature Importance Count Across All Patients')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.tight_layout()\n",
    "\n",
    "        if save_to_pdf:\n",
    "            plt.savefig(contribution_pdf_path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    if pca_plot:\n",
    "        combined_pca_df = pd.concat(all_pca_data, ignore_index=True)\n",
    "        combined_pca_df['Patient'] = all_patient_labels\n",
    "        pair_plot = sns.pairplot(combined_pca_df, hue='Patient', height=1.5, aspect=1, plot_kws={'s': 10}, diag_kind='hist')\n",
    "        pair_plot._legend.remove()\n",
    "        plt.suptitle(\"Combined PCA Pair Plot for All Patients\", y=1.02)\n",
    "\n",
    "        pca_image_path = output_pca_path.replace('.pdf', '_temp.png')\n",
    "        pair_plot.savefig(pca_image_path, dpi=300, bbox_inches='tight')\n",
    "        plt.show() \n",
    "        plt.close()\n",
    "\n",
    "        if save_to_pdf:\n",
    "            with PdfPages(output_pca_path) as pdf:\n",
    "                img = plt.imread(pca_image_path)\n",
    "                img_height, img_width = img.shape[0:2]\n",
    "                fig, ax = plt.subplots(figsize=(img_width / 100, img_height / 100)) \n",
    "                ax.imshow(img)\n",
    "                ax.axis('off') \n",
    "                pdf.savefig(fig, bbox_inches='tight') \n",
    "                plt.close(fig)\n",
    "\n",
    "        if os.path.exists(pca_image_path):\n",
    "            os.remove(pca_image_path)\n",
    "\n",
    "\n",
    "patients = ['p10','p11']\n",
    "\n",
    "# Example\n",
    "combined_plot_for_each_patient(patient_data, patients, n_components_pca=8, n_components_gmm=3, feature_importance=False, pca_plot=False)\n",
    "\n",
    "# Render and export all\n",
    "#combined_plot_for_each_patient(patient_data, patients, n_components_pca=8, n_components_gmm=3, save_to_pdf=True, feature_importance=True, pca_plot=True, output_pdf_path='PDF files\\\\All features\\\\Patientplots_1.pdf', contribution_pdf_path='PDF files\\\\All features\\\\Featureplots_1.pdf',output_pca_path = 'PDF files\\\\All features\\\\PCAplots_1.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette_scores(patient_data, patients, n_components_pca=5, n_components_gmm=3):\n",
    "    silhouette_scores = {}\n",
    "\n",
    "    for patient in patients:\n",
    "        data = patient_data[patient].copy()\n",
    "        data_for_pca = data.drop(columns=['Event'])\n",
    "\n",
    "        pca = PCA(n_components=n_components_pca)\n",
    "        reduced_data = pca.fit_transform(data_for_pca)\n",
    "\n",
    "        gmm = GaussianMixture(n_components=n_components_gmm, random_state=0)\n",
    "        gmm.fit(reduced_data)\n",
    "        labels = gmm.predict(reduced_data)\n",
    "\n",
    "        if len(set(labels)) > 1:\n",
    "            silhouette_avg = silhouette_score(reduced_data, labels)\n",
    "            silhouette_scores[patient] = silhouette_avg\n",
    "        else:\n",
    "            silhouette_scores[patient] = None\n",
    "\n",
    "    print(\"Silhouette Scores:\")\n",
    "    for patient, score in silhouette_scores.items():\n",
    "        if score is not None:\n",
    "            print(f\"Patient {patient}: {score:.2f}\")\n",
    "        else:\n",
    "            print(f\"Patient {patient}: Could not be computed (single cluster).\")\n",
    "\n",
    "    valid_scores = [score for score in silhouette_scores.values() if score is not None]\n",
    "\n",
    "    percentile_85 = np.percentile(valid_scores, 85) if valid_scores else None\n",
    "\n",
    "    patients_in_85th_percentile = [\n",
    "        patient for patient, score in silhouette_scores.items()\n",
    "        if score is not None and score >= percentile_85\n",
    "    ] if percentile_85 is not None else []\n",
    "\n",
    "    if percentile_85 is not None:\n",
    "        print(f\"\\n85th Percentile of Silhouette Scores: {percentile_85:.2f}\")\n",
    "        print(\"Patients in the 85th Percentile:\")\n",
    "        for patient in patients_in_85th_percentile:\n",
    "            print(f\"Patient {patient}\")\n",
    "    else:\n",
    "        print(\"\\n85th Percentile of Silhouette Scores: Not enough valid scores to compute.\")\n",
    "\n",
    "    return silhouette_scores, percentile_85, patients_in_85th_percentile\n",
    "\n",
    "patients = ['p10']\n",
    "silhouette_scores, percentile_85, patients_in_85th_percentile = calculate_silhouette_scores(patient_data, patients, n_components_pca=8, n_components_gmm=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test_responder_analysis(silhouette_scores, percentile_cutoff=85):\n",
    "    \"\"\"\n",
    "    Perform a t-test between responders (above the percentile cutoff) and non-responders.\n",
    "\n",
    "    Parameters:\n",
    "        silhouette_scores (dict): Dictionary of patient IDs and their silhouette scores.\n",
    "        percentile_cutoff (float): Percentile cutoff to separate responders and non-responders (default: 85).\n",
    "\n",
    "    Returns:\n",
    "        dict: Results of the t-test including means and p-value.\n",
    "    \"\"\"\n",
    "    valid_scores = {patient: score for patient, score in silhouette_scores.items() if score is not None}\n",
    "\n",
    "    cutoff_value = np.percentile(list(valid_scores.values()), percentile_cutoff)\n",
    "\n",
    "    responders = [score for score in valid_scores.values() if score >= cutoff_value]\n",
    "    non_responders = [score for score in valid_scores.values() if score < cutoff_value]\n",
    "\n",
    "    t_stat, p_value = ttest_ind(responders, non_responders, equal_var=False)\n",
    "\n",
    "    responders_mean = np.mean(responders) if responders else None\n",
    "    non_responders_mean = np.mean(non_responders) if non_responders else None\n",
    "\n",
    "    print(\"T-Test Results:\")\n",
    "    print(f\"Responders Mean (85th Percentile and above): {responders_mean:.2f}\")\n",
    "    print(f\"Non-Responders Mean (below 85th Percentile): {non_responders_mean:.2f}\")\n",
    "    print(f\"T-Statistic: {t_stat:.2f}\")\n",
    "    print(f\"P-Value: {p_value:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"responders_mean\": responders_mean,\n",
    "        \"non_responders_mean\": non_responders_mean,\n",
    "        \"t_statistic\": t_stat,\n",
    "        \"p_value\": p_value\n",
    "    }\n",
    "\n",
    "\n",
    "t_test_responder_analysis(silhouette_scores, percentile_cutoff=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_silhouette_scores(silhouette_scores, percentile_cutoff=85):\n",
    "    valid_scores = {patient: score for patient, score in silhouette_scores.items() if score is not None}\n",
    "\n",
    "    scores = list(valid_scores.values())\n",
    "    cutoff_value = np.percentile(scores, percentile_cutoff)\n",
    "\n",
    "    responders = {patient: score for patient, score in valid_scores.items() if score >= cutoff_value}\n",
    "    non_responders = {patient: score for patient, score in valid_scores.items() if score < cutoff_value}\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    plt.scatter(\n",
    "        list(responders.keys()),\n",
    "        list(responders.values()),\n",
    "        color='green',\n",
    "        label='Responders (85th Percentile and above)',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    plt.scatter(\n",
    "        list(non_responders.keys()),\n",
    "        list(non_responders.values()),\n",
    "        color='red',\n",
    "        label='Non-Responders (below 85th Percentile)',\n",
    "        s=100,\n",
    "        edgecolor='black'\n",
    "    )\n",
    "\n",
    "    plt.axhline(y=cutoff_value, color='blue', linestyle='--', label=f'85th Percentile Cutoff ({cutoff_value:.2f})')\n",
    "\n",
    "    plt.title('Silhouette Scores Visualization', fontsize=14)\n",
    "    plt.xlabel('Patients', fontsize=12)\n",
    "    plt.ylabel('Silhouette Score', fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_silhouette_scores(silhouette_scores, percentile_cutoff=85)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_resampling(silhouette_scores, n_iterations=1000, percentile_cutoff=85):\n",
    "    valid_scores = {patient: score for patient, score in silhouette_scores.items() if score is not None}\n",
    "\n",
    "    scores = list(valid_scores.values())\n",
    "    cutoff_value = np.percentile(scores, percentile_cutoff)\n",
    "\n",
    "    responders = [score for score in valid_scores.values() if score >= cutoff_value]\n",
    "    non_responders = [score for score in valid_scores.values() if score < cutoff_value]\n",
    "\n",
    "    if not responders or not non_responders:\n",
    "        raise ValueError(\"One of the groups (responders or non-responders) is empty. Cannot perform bootstrap resampling.\")\n",
    "\n",
    "    bootstrap_differences = []\n",
    "    for _ in range(n_iterations):\n",
    "        resampled_responders = [random.choice(responders) for _ in range(len(responders))]\n",
    "        resampled_non_responders = [random.choice(non_responders) for _ in range(len(non_responders))]\n",
    "\n",
    "        mean_difference = np.mean(resampled_responders) - np.mean(resampled_non_responders)\n",
    "        bootstrap_differences.append(mean_difference)\n",
    "\n",
    "    lower_bound = np.percentile(bootstrap_differences, 2.5)\n",
    "    upper_bound = np.percentile(bootstrap_differences, 97.5)\n",
    "\n",
    "    return {\n",
    "        \"mean_difference\": np.mean(bootstrap_differences),\n",
    "        \"95_ci\": (lower_bound, upper_bound),\n",
    "        \"bootstrap_differences\": bootstrap_differences\n",
    "    }\n",
    "\n",
    "bootstrap_results = bootstrap_resampling(silhouette_scores, n_iterations=1000, percentile_cutoff=85)\n",
    "print(\"Bootstrap Results:\")\n",
    "print(f\"Mean Difference: {bootstrap_results['mean_difference']:.4f}\")\n",
    "print(f\"95% Confidence Interval: {bootstrap_results['95_ci']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_silhouette_scores(silhouette_scores, percentile_cutoff=85):\n",
    "    valid_scores = {patient: score for patient, score in silhouette_scores.items() if score is not None}\n",
    "\n",
    "    scores = list(valid_scores.values())\n",
    "    cutoff_value = np.percentile(scores, percentile_cutoff)\n",
    "\n",
    "    patient_labels = {\n",
    "        patient: 'Responder' if score >= cutoff_value else 'Non-Responder'\n",
    "        for patient, score in valid_scores.items()\n",
    "    }\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    colors = ['green' if score >= cutoff_value else 'red' for score in valid_scores.values()]\n",
    "    plt.scatter(\n",
    "        list(valid_scores.keys()),\n",
    "        list(valid_scores.values()),\n",
    "        c=colors,\n",
    "        s=100,\n",
    "        edgecolor='black',\n",
    "        label='Patients'\n",
    "    )\n",
    "\n",
    "    plt.axhline(y=cutoff_value, color='blue', linestyle='--', label=f'85th Percentile Cutoff ({cutoff_value:.2f})')\n",
    "\n",
    "    plt.title('Silhouette Scores Visualization', fontsize=14)\n",
    "    plt.xlabel('Patients', fontsize=12)\n",
    "    plt.ylabel('Silhouette Score', fontsize=12)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return patient_labels\n",
    "\n",
    "patient_labels = visualize_silhouette_scores(silhouette_scores, percentile_cutoff=85)\n",
    "\n",
    "print(\"\\nPatient Labels:\")\n",
    "for patient, label in patient_labels.items():\n",
    "    print(f\"{patient}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_responder_labels(silhouette_scores, percentile_cutoff=85):\n",
    "    valid_scores = {patient: score for patient, score in silhouette_scores.items() if score is not None}\n",
    "\n",
    "    scores = list(valid_scores.values())\n",
    "    cutoff_value = np.percentile(scores, percentile_cutoff)\n",
    "\n",
    "    patient_labels = {\n",
    "        patient: 'Responder' if score >= cutoff_value else 'Non-Responder'\n",
    "        for patient, score in valid_scores.items()\n",
    "    }\n",
    "\n",
    "    print(f\"85th Percentile Cutoff Value: {cutoff_value:.4f}\")\n",
    "    print(\"Patient Labels:\")\n",
    "    for patient, label in patient_labels.items():\n",
    "        print(f\"{patient}: {label}\")\n",
    "\n",
    "    return patient_labels\n",
    "\n",
    "\n",
    "patient_labels = assign_responder_labels(silhouette_scores, percentile_cutoff=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_logistic_classifier(patient_data, patient_labels, n_pca_components=5):\n",
    "    X = []\n",
    "    y = []\n",
    "    for patient, label in patient_labels.items():\n",
    "        if patient in patient_data:\n",
    "            X.append(patient_data[patient].drop(columns=['Event'], errors='ignore').values)\n",
    "            y.extend([label] * len(patient_data[patient]))\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    clf = LogisticRegression(class_weight='balanced')\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"Logistic Regression Classifier Results:\\n\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "create_logistic_classifier(patient_data, patient_labels, n_pca_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lightgbm_classifier(patient_data, patient_labels, n_pca_components=5, n_estimators=100):\n",
    "    X = []\n",
    "    y = []\n",
    "    for patient, label in patient_labels.items():\n",
    "        if patient in patient_data:\n",
    "            X.append(patient_data[patient].drop(columns=['Event'], errors='ignore').values)\n",
    "            y.extend([label] * len(patient_data[patient]))\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    print(f\"Class 0 count: {np.sum(y == 0)}, Class 1 count: {np.sum(y == 1)}\")\n",
    "    print(f\"Shape of X: {X.shape}, Shape of y: {y.shape}\")\n",
    "\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    clf = LGBMClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(\"LightGBM Classifier Results:\\n\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "create_lightgbm_classifier(patient_data, patient_labels, n_pca_components=8, n_estimators=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network(patient_data, patient_labels, n_pca_components=5, epochs=50, batch_size=32, learning_rate=0.001):\n",
    "    X = []\n",
    "    y = []\n",
    "    for patient, label in patient_labels.items():\n",
    "        if patient in patient_data:\n",
    "            X.append(patient_data[patient].drop(columns=['Event'], errors='ignore').values)\n",
    "            y.extend([label] * len(patient_data[patient]))\n",
    "\n",
    "    X = np.vstack(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    pca = PCA(n_components=n_pca_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    class ResponderClassifier(nn.Module):\n",
    "        def __init__(self, input_size):\n",
    "            super(ResponderClassifier, self).__init__()\n",
    "            self.model = nn.Sequential(\n",
    "                nn.Linear(input_size, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(64, 32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2),\n",
    "                nn.Linear(32, 1)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x)\n",
    "\n",
    "    model = ResponderClassifier(input_size=n_pca_components)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([len(y_train[y_train == 0]) / len(y_train[y_train == 1])], dtype=torch.float32))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            outputs = model(inputs).squeeze()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            y_pred.extend(predictions.numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    print(\"\\nNeural Network Classifier Results:\\n\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_true, y_pred))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "create_neural_network(patient_data, patient_labels, n_pca_components=8, epochs=10, batch_size=16, learning_rate=0.001)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
