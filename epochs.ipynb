{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages: minimize for better structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from matplotlib.patches import Patch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage of Epochs to classify Responders/Non Responder\n",
    "\n",
    "This is a notebook to map and structure the use of classifying nearby epochs, in order to make more accurate classifications. Hence, it does a sandbox for epoch-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Idea: (aim -> individual level)\n",
    "- Initialize the data\n",
    "- Use epochs? Maybe this should be during the training proccess or data initialization?\n",
    "- These epochs are classified to be responder or non responder. \n",
    "  - How is this done? Maybe start with a simple architecture so it trains fast for testing\n",
    "  - It has to be unsupervised. Labels aren't given yet.\n",
    "  - These assigned labels has to go through the threshold in order to pass. (visualize this to evaluate)\n",
    "        - A value is chosen to be set as a treshold for how many seconds from literature\n",
    "        - It could be supervised. By running unsupervised and using the approximated labels to train a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Initialize the data, and make sure this is done correct:\n",
    "Explanation:\n",
    "\n",
    "To begin, a folder directory and path are specified to access the files. These paths are joined using the os.path.join() function to handle the paths correctly and ensure they point to the right destination. Empty arrays are initialized for storing values. For instance, the patient_numbers list stores the IDs for each patient, which could be formatted like p1, p2, and so on.\n",
    "\n",
    "A for loop is then used to iterate over each file in the directory, which corresponds to iterating through each patient. The filenames are processed using the os.fsdecode() function, and the patient number is extracted by splitting the filename based on underscores and selecting the first element. The patient number is then added to the patient_numbers array that was initialized earlier.\n",
    "\n",
    "Next, the full directory of each patient file is constructed by joining the base path with the specific filename. This ensures that the data can be properly accessed. The data itself is read from the CSV files using pd.read_csv(), as the files are in CSV format.\n",
    "\n",
    "After loading the data, some preprocessing steps are applied. Column names are adjusted, and any text-based attributes are converted into numerical values for easier handling. A set of specific columns is chosen for standardization, which is done using the StandardScaler from sklearn. This step ensures that features are on the same scale, preventing large values from dominating the training process. By standardizing, the mean of each column is set to 0, and the standard deviation is set to 1, ensuring uniformity in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys in patient_data dictionary: ['p10', 'p11', 'p12', 'p13', 'p14', 'p15', 'p16', 'p17', 'p18', 'p19', 'p20', 'p21', 'p22', 'p23', 'p24', 'p25', 'p27', 'p28', 'p29', 'p2', 'p30', 'p31', 'p32', 'p33', 'p34', 'p35', 'p36', 'p37', 'p38', 'p39', 'p3', 'p40', 'p41', 'p42', 'p43', 'p44', 'p45', 'p46', 'p47', 'p48', 'p49', 'p4', 'p50', 'p51', 'p52', 'p53', 'p54', 'p56', 'p57', 'p58', 'p59', 'p5', 'p61', 'p62', 'p63', 'p65', 'p66', 'p67', 'p68', 'p69', 'p6', 'p71', 'p72', 'p73', 'p74', 'p75', 'p76', 'p77', 'p78', 'p79', 'p7', 'p80', 'p8', 'p9']\n"
     ]
    }
   ],
   "source": [
    "path = 'CSV_features_NEW'\n",
    "folder = 'C:\\\\Users\\\\RJEN0307\\\\Desktop\\\\Bachelorprojekt\\\\Bachelor_project_2024\\\\'\n",
    "\n",
    "# Combine them using os.path.join for proper path handling\n",
    "full_path = os.path.join(folder, path)\n",
    "#print(os.listdir(full_path))\n",
    "\n",
    "\n",
    "patient_numbers = []\n",
    "patient_data = {}\n",
    "\n",
    "for file in os.listdir(full_path):\n",
    "    filename = os.fsdecode(file)\n",
    "    patient_number = filename.split('_')[0]  # This will give 'p3' from 'p3_features.csv'\n",
    "    patient_numbers.append(patient_number)\n",
    "    patient_file_dir = os.path.join(full_path, filename)\n",
    "    data = pd.read_csv(patient_file_dir)\n",
    "    data.rename(columns={'Unnamed: 0': 'Index'}, inplace=True)\n",
    "    data['Event'] = data['Event'].map({'R': 0, 'M': 1, 'F': 2})\n",
    "    standarize_list = ['PSD Delta', 'PSD Delta_N', 'PSD Theta', 'PSD Theta_N', 'PSD Alpha', 'PSD Alpha_N', 'PSD Beta', 'PSD Beta_N', 'PSD Gamma', 'PSD Gamma_N', 'PSD SE', 'PSD MSF', 'PSD Sef90', 'PSD Sef95', 'PE', 'wSMI', 'Kolmogorov', 'Mean RR', 'Std RR', 'Mean HR', 'Std HR', 'Min HR', 'Max HR', 'Freq_Slope mean', 'Freq_Slope std']\n",
    "\n",
    "    sklearn.preprocessing.StandardScaler().set_output(transform='pandas') \n",
    "    data[standarize_list] = sklearn.preprocessing.StandardScaler().fit(data[standarize_list]).transform(data[standarize_list])\n",
    "\n",
    "    patient_data[patient_number] = data\n",
    "\n",
    "#print(patient_numbers)\n",
    "#print(data)\n",
    "print(\"Keys in patient_data dictionary:\", list(patient_data.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Initialize the Unsupervised learning\n",
    "This is the most important step. Here, the model gets its performance from.\n",
    "\n",
    "For testing purpose, a simple model is first initialized. It is important that it looks at each patient, and not all the data at the same time. Since the initialization of the data is structured such that it stores each patients data seperately in the patient_data dictionary, the values can be extract from this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use case (maybe delete this)\n",
    "for patient in patient_data.keys():\n",
    "    print(f\"Patient: {patient}\")\n",
    "    print(patient_data[patient].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patient in patient_data.keys():\n",
    "    a = 1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
